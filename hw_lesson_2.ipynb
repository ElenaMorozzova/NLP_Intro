{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "advance-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "drawn-adelaide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>{drag, dysfunct, kid, run, father, selfish}</td>\n",
       "      <td>{drag, kid, run, dysfunctional, dysfunction, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>{caus, pdx, wheelchair, offer, disapoint, gett...</td>\n",
       "      <td>{getthanked, pdx, disapointed, wheelchair, off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>{bihday, majesti}</td>\n",
       "      <td>{bihday, majesty}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>model love take with all the time in ur</td>\n",
       "      <td>[model, love, take, with, all, the, time, in, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>{take, model, love, ur, time}</td>\n",
       "      <td>{take, model, love, ur, time}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>{factsguid, societi, motiv}</td>\n",
       "      <td>{factsguide, motivation, society}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  when father is dysfunctional and is so selfish...   \n",
       "1   2    0.0  thanks for lyft credit cannot use cause they d...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0            model love take with all the time in ur   \n",
       "4   5    0.0                  factsguide society now motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, take, with, all, the, time, in, ur]   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0        {drag, dysfunct, kid, run, father, selfish}   \n",
       "1  {caus, pdx, wheelchair, offer, disapoint, gett...   \n",
       "2                                  {bihday, majesti}   \n",
       "3                      {take, model, love, ur, time}   \n",
       "4                        {factsguid, societi, motiv}   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  {drag, kid, run, dysfunctional, dysfunction, f...  \n",
       "1  {getthanked, pdx, disapointed, wheelchair, off...  \n",
       "2                                  {bihday, majesty}  \n",
       "3                      {take, model, love, ur, time}  \n",
       "4                  {factsguide, motivation, society}  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('tweet_data.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suited-sculpture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49159 entries, 0 to 49158\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   id                    49159 non-null  int64  \n",
      " 1   label                 31962 non-null  float64\n",
      " 2   tweet                 49159 non-null  object \n",
      " 3   tweet_token           49159 non-null  object \n",
      " 4   tweet_token_filtered  49159 non-null  object \n",
      " 5   tweet_stemmed         49159 non-null  object \n",
      " 6   tweet_lemmatized      49159 non-null  object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 3.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-wichita",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "\n",
    "Создайте мешок слов с помощью\n",
    "sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed'\n",
    "и 'tweet_lemmatized' отдельно.\n",
    "\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с\n",
    "помощью max_df.\n",
    "- Ограничим количество слов, попадающий в мешок, с помощью max_features =1000.\n",
    "- Исключим стоп-слова с помощью stop_words='english'.\n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "disciplinary-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_stemmed = CountVectorizer(stop_words='english', max_df=0.9, max_features=1000)\n",
    "count_vectorizer_lemmatized = CountVectorizer(stop_words='english', max_df=0.9, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "posted-checkout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drag dysfunct kid run father selfish',\n",
       " 'caus pdx wheelchair offer disapoint getthank credit use lyft thank van',\n",
       " 'bihday majesti',\n",
       " 'take model love ur time',\n",
       " 'factsguid societi motiv',\n",
       " 'get chao big allshowandnogo fare fan disput leav pay huge talk',\n",
       " 'tomorrow camp danni',\n",
       " 'revolutionschool school imagin exam year hate think next actorslif girl',\n",
       " 'clevelandcavali champion land love cav allin cleveland',\n",
       " 'welcom gr']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: \" \".join(x), data['tweet_stemmed'].values[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "diverse-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_stemmed = count_vectorizer_stemmed.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_stemmed'].values)))\n",
    "bag_of_words_lemmatized = count_vectorizer_lemmatized.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_lemmatized'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amateur-integral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49159x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 201762 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beautiful-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49159x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 190180 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floating-somewhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "2    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "3    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "4    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0     0     0          0   0     0     0      0       0   0      0  \n",
       "1     0     0          0   0     0     0      0       0   0      0  \n",
       "2     0     0          0   0     0     0      0       0   0      0  \n",
       "3     0     0          0   0     0     0      0       0   0      0  \n",
       "4     0     0          0   0     0     0      0       0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count_vect_stemmed = pd.DataFrame(bag_of_words_stemmed.toarray(), columns=count_vectorizer_stemmed.get_feature_names())\n",
    "df_count_vect_stemmed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "horizontal-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  accept  account  act  action  actor  actually  adapt  \\\n",
       "0     0           0       0        0    0       0      0         0      0   \n",
       "1     0           0       0        0    0       0      0         0      0   \n",
       "2     0           0       0        0    0       0      0         0      0   \n",
       "3     0           0       0        0    0       0      0         0      0   \n",
       "4     0           0       0        0    0       0      0         0      0   \n",
       "\n",
       "   add  ...  yes  yesterday  yo  yoga  york  young  youtube  yr  yrs  yummy  \n",
       "0    0  ...    0          0   0     0     0      0        0   0    0      0  \n",
       "1    0  ...    0          0   0     0     0      0        0   0    0      0  \n",
       "2    0  ...    0          0   0     0     0      0        0   0    0      0  \n",
       "3    0  ...    0          0   0     0     0      0        0   0    0      0  \n",
       "4    0  ...    0          0   0     0     0      0        0   0    0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count_vect_lemmatized = pd.DataFrame(bag_of_words_lemmatized.toarray(), columns=count_vectorizer_lemmatized.get_feature_names())\n",
    "df_count_vect_lemmatized.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-seminar",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "Создайте мешок слов с помощью\n",
    "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и\n",
    "'tweet_lemmatized' отдельно.\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "- Исключим стоп-слова с помощью stop_words='english'.\n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "israeli-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_stemmed = TfidfVectorizer(stop_words='english', max_df=0.9, max_features=1000)\n",
    "tfidf_vectorizer_lemmatized = TfidfVectorizer(stop_words='english', max_df=0.9, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wound-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_tfidf_stemmed = tfidf_vectorizer_stemmed.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_stemmed'].values)))\n",
    "bag_of_words_tfidf_lemmatized = tfidf_vectorizer_lemmatized.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_lemmatized'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "distant-bibliography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49159x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 201762 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_tfidf_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efficient-niger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49159x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 190180 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_tfidf_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "controlled-bikini",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n",
       "0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "2  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "3  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "4  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "\n",
       "   yeah  year  yesterday   yo  yoga  york  young  youtub   yr  yummi  \n",
       "0   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "1   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "2   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "3   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "4   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_vect_stemmed = pd.DataFrame(bag_of_words_tfidf_stemmed.toarray(), columns=tfidf_vectorizer_stemmed.get_feature_names())\n",
    "df_tfidf_vect_stemmed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "inclusive-ethnic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  accept  account  act  action  actor  actually  adapt  \\\n",
       "0   0.0         0.0     0.0      0.0  0.0     0.0    0.0       0.0    0.0   \n",
       "1   0.0         0.0     0.0      0.0  0.0     0.0    0.0       0.0    0.0   \n",
       "2   0.0         0.0     0.0      0.0  0.0     0.0    0.0       0.0    0.0   \n",
       "3   0.0         0.0     0.0      0.0  0.0     0.0    0.0       0.0    0.0   \n",
       "4   0.0         0.0     0.0      0.0  0.0     0.0    0.0       0.0    0.0   \n",
       "\n",
       "   add  ...  yes  yesterday   yo  yoga  york  young  youtube   yr  yrs  yummy  \n",
       "0  0.0  ...  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0  0.0    0.0  \n",
       "1  0.0  ...  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0  0.0    0.0  \n",
       "2  0.0  ...  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0  0.0    0.0  \n",
       "3  0.0  ...  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0  0.0    0.0  \n",
       "4  0.0  ...  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_vect_stemmed = pd.DataFrame(bag_of_words_tfidf_lemmatized.toarray(), columns=tfidf_vectorizer_lemmatized.get_feature_names())\n",
    "df_tfidf_vect_stemmed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-buffer",
   "metadata": {},
   "source": [
    "### Задание 3. \n",
    "Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "known-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import SparsePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secret-electricity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corp = open('corpus.txt').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data_corp.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "# создаем df\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "certified-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "shared-toddler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_count, train_y)\n",
    "predictions = classifier.predict(xvalid_count)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rolled-stock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8668"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faced-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C векторайзером\n",
    "from itertools import chain\n",
    "tokenized_list = list(data.tweet_stemmed.values)\n",
    "tokenized_list = list(chain(*tokenized_list))\n",
    "\n",
    "lemmatized_list = list(data.tweet_lemmatized.values)\n",
    "lemmatized_list = list(chain(*lemmatized_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "jewish-springfield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer tweet_stemmed\n",
    "count_vect = CountVectorizer(ngram_range=(1, 1), analyzer='word', stop_words='english', lowercase=False,\n",
    "                                   binary=False, max_df = 0.9, max_features = 4000)\n",
    "count_vect.fit(tokenized_list)\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_count, train_y)\n",
    "predictions = classifier.predict(xvalid_count)\n",
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "superior-hughes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer tweet_lemmatized\n",
    "count_vect = CountVectorizer(ngram_range=(1, 1), analyzer='word', stop_words='english', lowercase=False,\n",
    "                                   binary=False, max_df = 0.9, max_features = 4000)\n",
    "count_vect.fit(lemmatized_list)\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_count, train_y)\n",
    "predictions = classifier.predict(xvalid_count)\n",
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "civilian-prince",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer tweet_stemmed\n",
    "count_vect = TfidfVectorizer(max_df=0.9, max_features = 4000, stop_words='english', lowercase=False,\n",
    "                                  analyzer='word')\n",
    "count_vect.fit(tokenized_list)\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_count, train_y)\n",
    "predictions = classifier.predict(xvalid_count)\n",
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cleared-david",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7916"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer tweet_lemmatized\n",
    "count_vect = TfidfVectorizer(max_df=0.9, max_features = 4000, stop_words='english', lowercase=False,\n",
    "                                  analyzer='word')\n",
    "count_vect.fit(lemmatized_list)\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_count, train_y)\n",
    "predictions = classifier.predict(xvalid_count)\n",
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-violence",
   "metadata": {},
   "source": [
    "Качество модели чуть лучше при применении  tweet_lemmatized tfidf. \n",
    "Попробуем подобрать параметры tweet_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "certified-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_df = [0.5, 0.7, 0.8, 0.9, 1]\n",
    "n_features = [500, 700, 1000, 2000, 3000, 4000, 5000, 6000]\n",
    "stop_w = ['english', None]\n",
    "log_max_df = []\n",
    "log_features = []\n",
    "log_acc = []\n",
    "log_acc2 = []\n",
    "log_stop_w = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "therapeutic-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_df in n_max_df:\n",
    "    for max_features in n_features:\n",
    "        for w in stop_w:\n",
    "            # CountVectorizer\n",
    "            count_vect = CountVectorizer(ngram_range=(1, 1), analyzer='word', stop_words=w, lowercase=False,\n",
    "                                   binary=False, max_df = max_df, max_features = max_features)\n",
    "                       \n",
    "            count_vect.fit(lemmatized_list)\n",
    "            xtrain_count =  count_vect.transform(train_x)\n",
    "            xvalid_count =  count_vect.transform(valid_x)\n",
    "              \n",
    "            classifier = linear_model.LogisticRegression()\n",
    "            classifier.fit(xtrain_count, train_y)\n",
    "            predictions = classifier.predict(xvalid_count)\n",
    "            log_acc.append(accuracy_score(valid_y, predictions))\n",
    "            \n",
    "            #  Tfidf\n",
    "            count_vect = TfidfVectorizer(max_df=max_df, max_features = max_features, stop_words=w, lowercase=False,\n",
    "                                  analyzer='word')\n",
    "            count_vect.fit(lemmatized_list)\n",
    "            \n",
    "            xtrain_count =  count_vect.transform(train_x)\n",
    "            xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "            classifier = linear_model.LogisticRegression()\n",
    "            classifier.fit(xtrain_count, train_y)\n",
    "            predictions = classifier.predict(xvalid_count)\n",
    "            log_acc2.append(accuracy_score(valid_y, predictions))\n",
    "            \n",
    "            log_max_df.append(max_df)\n",
    "            log_features.append(max_features)\n",
    "            log_stop_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "mental-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = pd.DataFrame(log_acc, columns = ['accuracy_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "incredible-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = pd.DataFrame(log_acc2, columns = ['accuracy_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "federal-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1['accuracy'] = log_acc\n",
    "res1['max_df'] = log_max_df\n",
    "res1['features'] = log_features\n",
    "res1['stop_w'] = log_stop_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "changed-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2['accuracy'] = log_acc2\n",
    "res2['max_df'] = log_max_df\n",
    "res2['features'] = log_features\n",
    "res2['stop_w'] = log_stop_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "appointed-choice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_count</th>\n",
       "      <th>accuracy_tfidf</th>\n",
       "      <th>max_df</th>\n",
       "      <th>features</th>\n",
       "      <th>stop_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy_count  accuracy_tfidf  max_df  features stop_w\n",
       "187          0.7876          0.7944     0.7      4000   None\n",
       "123          0.7876          0.7944     0.8      4000   None\n",
       "107          0.7876          0.7944     0.7      4000   None\n",
       "219          0.7876          0.7944     0.9      4000   None\n",
       "203          0.7876          0.7944     0.8      4000   None\n",
       "171          0.7876          0.7944     0.5      4000   None\n",
       "139          0.7876          0.7944     0.9      4000   None\n",
       "189          0.7900          0.7952     0.7      5000   None\n",
       "173          0.7900          0.7952     0.5      5000   None\n",
       "125          0.7900          0.7952     0.8      5000   None\n",
       "141          0.7900          0.7952     0.9      5000   None\n",
       "109          0.7900          0.7952     0.7      5000   None\n",
       "205          0.7900          0.7952     0.8      5000   None\n",
       "93           0.7900          0.7952     0.5      5000   None\n",
       "221          0.7900          0.7952     0.9      5000   None"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sort_values(by=['accuracy_count']).tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "drawn-technology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_count</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_df</th>\n",
       "      <th>features</th>\n",
       "      <th>stop_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.7948</td>\n",
       "      <td>0.7948</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.7948</td>\n",
       "      <td>0.7948</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.7948</td>\n",
       "      <td>0.7948</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy_count  accuracy  max_df  features stop_w\n",
       "207          0.7948    0.7948     0.8      6000   None\n",
       "15           0.7948    0.7948     0.5      6000   None\n",
       "127          0.7948    0.7948     0.8      6000   None\n",
       "45           0.7952    0.7952     0.8      5000   None\n",
       "13           0.7952    0.7952     0.5      5000   None\n",
       "221          0.7952    0.7952     0.9      5000   None\n",
       "205          0.7952    0.7952     0.8      5000   None\n",
       "189          0.7952    0.7952     0.7      5000   None\n",
       "93           0.7952    0.7952     0.5      5000   None\n",
       "61           0.7952    0.7952     0.9      5000   None\n",
       "29           0.7952    0.7952     0.7      5000   None\n",
       "125          0.7952    0.7952     0.8      5000   None\n",
       "109          0.7952    0.7952     0.7      5000   None\n",
       "141          0.7952    0.7952     0.9      5000   None\n",
       "173          0.7952    0.7952     0.5      5000   None"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.sort_values(by=['accuracy_count']).tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-stream",
   "metadata": {},
   "source": [
    "Несколько лучше результаты получились при max_features = 5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
